(window.webpackJsonp=window.webpackJsonp||[]).push([[69],{745:function(a,e,n){"use strict";n.r(e);var s=n(4),v=Object(s.a)({},(function(){var a=this,e=a.$createElement,n=a._self._c||e;return n("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[n("h2",{attrs:{id:"大数据"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#大数据"}},[a._v("#")]),a._v(" 大数据")]),a._v(" "),n("p",[a._v("大数据的4V特征（volume，variety，value，velocity），即大量，多样性，价值，及时性。")]),a._v(" "),n("ol",[n("li",[a._v("数据体量巨大（这是大数据最明显的特征），有人认为，大数据的起始计量单位至少是P（1000个T）、E（100万个T）或Z（10亿个T）；这里按顺序给出所有单位：bit、Byte、KB、MB、GB、TB、PB、EB、ZB、YB、BB、NB、DB（进率2^10）。")]),a._v(" "),n("li",[a._v("数据类型繁多（也就是多维度的表现形式）。比如，网络日志、视频、图片、地理位置信息等等。")]),a._v(" "),n("li",[a._v("价值密度低，商业价值高。以视频为例，一小时的视频，在不间断的监控过程中，可能有用的数据仅仅只有一两秒。因此，如何结合业务逻辑并通过强大的机器算法来挖掘数据价值（所谓“浪里淘金”吧），是最需要解决的问题。")]),a._v(" "),n("li",[a._v("处理速度快且及时。数据处理遵循“1秒定律”，可从各种类型的数据中快速获得高价值的信息。")])]),a._v(" "),n("h2",{attrs:{id:"大数据技术"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#大数据技术"}},[a._v("#")]),a._v(" 大数据技术")]),a._v(" "),n("p",[a._v("一、大数据采集大数据采集，即对各种来源的结构化和非结构化海量数据，所进行的采集。")]),a._v(" "),n("p",[a._v("数据库采集：流行的有Sqoop和ETL，传统的关系型数据库MySQL和Oracle 也依然充当着许多企业的数据存储方式。当然了，目前对于开源的Kettle和Talend本身，也集成了大数据集成内容，可实现hdfs，hbase和主流Nosq数据库之间的数据同步和集成。")]),a._v(" "),n("div",{staticClass:"language- line-numbers-mode"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[a._v("ETL，Extraction-Transformation-Loading的缩写，中文名为数据抽取、转换和加载。ETL负责将分布的、异构数据源中的数据如关系数据、平面数据文件等抽取到临时中间层后进行清洗、转换、集成，最后加载到数据仓库或数据集市中，成为联机分析处理、数据挖掘的基础。ETL是BI项目最重要的一个环节，通常情况下ETL会花掉整个项目的1/3的时间，ETL设计的好坏直接关接到BI项目的成败。ETL也是一个长期的过程，只有不断的发现问题并解决问题，才能使ETL运行效率更高，为项目后期开发提供准确的数据。\n")])]),a._v(" "),n("div",{staticClass:"line-numbers-wrapper"},[n("span",{staticClass:"line-number"},[a._v("1")]),n("br")])]),n("p",[a._v("网络数据采集：一种借助网络爬虫或网站公开API，从网页获取非结构化或半结构化数据，并将其统一结构化为本地数据的数据采集方式。")]),a._v(" "),n("p",[a._v("文件采集：包括实时文件采集和处理技术flume、基于ELK的日志采集和增量采集等等。")]),a._v(" "),n("p",[a._v("二、大数据预处理")]),a._v(" "),n("p",[a._v("大数据预处理，指的是在进行数据分析之前，先对采集到的原始数据所进行的诸如“清洗、填补、平滑、合并、规格化、一致性检验”等一系列操作，旨在提高数据质量，为后期分析工作奠定基础。数据预处理主要包括四个部分：数据清理、数据集成、数据转换、数据规约。")]),a._v(" "),n("p",[a._v("数据清理：指利用ETL等清洗工具，对有遗漏数据(缺少感兴趣的属性)、噪音数据(数据中存在着错误、或偏离期望值的数据)、不一致数据进行处理。\n数据集成：是指将不同数据源中的数据，合并存放到统一数据库的，存储方法，着重解决三个问题：模式匹配、数据冗余、数据值冲突检测与处理。\n数据转换：是指对所抽取出来的数据中存在的不一致，进行处理的过程。它同时包含了数据清洗的工作，即根据业务规则对异常数据进行清洗，以保证后续分析结果准确性。\n数据规约：是指在最大限度保持数据原貌的基础上，最大限度精简数据量，以得到较小数据集的操作，包括：数据方聚集、维规约、数据压缩、数值规约、概念分层等。")]),a._v(" "),n("p",[a._v("三、大数据存储")]),a._v(" "),n("p",[a._v("大数据存储，指用存储器，以数据库的形式，存储采集到的数据的过程，包含三种典型路线：")]),a._v(" "),n("p",[a._v("1、基于MPP架构的新型数据库集群\n采用Shared Nothing架构，结合MPP架构的高效分布式计算模式，通过列存储、粗粒度索引等多项大数据处理技术，重点面向行业大数据所展开的数据存储方式。具有低成本、高性能、高扩展性等特点，在企业分析类应用领域有着广泛的应用。较之传统数据库，其基于MPP产品的PB级数据分析能力，有着显著的优越性。自然，MPP数据库，也成为了企业新一代数据仓库的最佳选择。")]),a._v(" "),n("div",{staticClass:"language- line-numbers-mode"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[a._v("MPP (Massively Parallel Processing)，即大规模并行处理。简单来说，MPP是将任务并行的分散到多个服务器和节点上，在每个节点上计算完成后，将各自部分的结果汇总在一起得到最终的结果(与Hadoop相似)。\n\nHadoop在处理非结构化和半结构化数据上具备优势，尤其适合海量数据批处理等应用要求。\nMPP适合替代现有关系数据机构下的大数据处理，具有较高的效率。\n")])]),a._v(" "),n("div",{staticClass:"line-numbers-wrapper"},[n("span",{staticClass:"line-number"},[a._v("1")]),n("br"),n("span",{staticClass:"line-number"},[a._v("2")]),n("br"),n("span",{staticClass:"line-number"},[a._v("3")]),n("br"),n("span",{staticClass:"line-number"},[a._v("4")]),n("br")])]),n("p",[a._v("2、基于Hadoop的技术扩展和封装基于Hadoop的技术扩展和封装，是针对传统关系型数据库难以处理的数据和场景（针对非结构化数据的存储和计算等），利用Hadoop开源优势及相关特性（善于处理非结构、半结构化数据、复杂的ETL流程、复杂的数据挖掘和计算模型等），衍生出相关大数据技术的过程。伴随着技术进步，其应用场景也将逐步扩大，目前最为典型的应用场景：通过扩展和封装 Hadoop来实现对互联网大数据存储、分析的支撑，其中涉及了几十种NoSQL技术。")]),a._v(" "),n("p",[a._v("3、大数据一体机这是一种专为大数据的分析处理而设计的软、硬件结合的产品。它由一组集成的服务器、存储设备、操作系统、数据库管理系统，以及为数据查询、处理、分析而预安装和优化的软件组成，具有良好的稳定性和纵向扩展性。")]),a._v(" "),n("p",[a._v("四、大数据分析挖掘")]),a._v(" "),n("p",[a._v("从可视化分析、数据挖掘算法、预测性分析、语义引擎、数据质量管理等方面，对杂乱无章的数据，进行萃取、提炼和分析的过程。")]),a._v(" "),n("p",[a._v("1、可视化\n分析可视化分析，指借助图形化手段，清晰并有效传达与沟通信息的分析手段。主要应用于海量数据关联分析，即借助可视化数据分析平台，对分散异构数据进行关联分析，并做出完整分析图表的过程。具有简单明了、清晰直观、易于接受的特点。")]),a._v(" "),n("p",[a._v("2、数据挖掘算法\n数据挖掘算法，即通过创建数据挖掘模型，而对数据进行试探和计算的，数据分析手段。它是大数据分析的理论核心。数据挖掘算法多种多样，且不同算法因基于不同的数据类型和格式，会呈现出不同的数据特点。但一般来讲，创建模型的过程却是相似的，即首先分析用户提供的数据，然后针对特定类型的模式和趋势进行查找，并用分析结果定义创建挖掘模型的最佳参数，并将这些参数应用于整个数据集，以提取可行模式和详细统计信息。")]),a._v(" "),n("p",[a._v("3、预测性分析\n预测性分析，是大数据分析最重要的应用领域之一，通过结合多种高级分析功能（特别统计分析、预测建模、数据挖掘、文本分析、实体分析、优化、实时评分、机器学习等），达到预测不确定事件的目的。帮助分用户析结构化和非结构化数据中的趋势、模式和关系，并运用这些指标来预测将来事件，为采取措施提供依据。")]),a._v(" "),n("p",[a._v("4、语义引擎\n语义引擎，指通过为已有数据添加语义的操作，提高用户互联网搜索体验。")]),a._v(" "),n("p",[a._v("5、数据质量管理指对数据全生命周期的每个阶段（计划、获取、存储、共享、维护、应用、消亡等）中可能引发的各类数据质量问题，进行识别、度量、监控、预警等操作，以提高数据质量的一系列管理活动。")]),a._v(" "),n("p",[a._v("以上是从大的方面来讲，具体来说大数据的框架技术有很多，这里列举其中一些：")]),a._v(" "),n("p",[a._v("文件存储：Hadoop HDFS、Tachyon、KFS\n离线计算：Hadoop MapReduce、Spark\n流式、实时计算：Storm、Spark Streaming、S4、Heron\nK-V、NOSQL数据库：HBase、Redis、MongoDB\n资源管理：YARN、Mesos、k8s\n日志收集：Flume、Scribe、Logstash、Kibana\n消息系统：Kafka、StormMQ、ZeroMQ、RabbitMQ\n查询分析：Hive、Impala、Pig、Presto、Phoenix、SparkSQL、Drill、Flink、Kylin、Druid\n分布式协调服务：Zookeeper\n集群管理与监控：Ambari、Ganglia、Nagios、Cloudera Manager\n数据挖掘、机器学习：Mahout、Spark MLLib\n数据同步：Sqoop\n任务调度：Oozie")])])}),[],!1,null,null,null);e.default=v.exports}}]);